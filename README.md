# ğŸ¬ BuzzUGC - AI Video Creator

<div align="center">

**Transform any avatar into a speaking video with Google's VEO 3 Fast model**

[![React](https://img.shields.io/badge/React-19.1.1-blue.svg)](https://reactjs.org/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.8.2-blue.svg)](https://www.typescriptlang.org/)
[![Vite](https://img.shields.io/badge/Vite-6.2.0-646CFF.svg)](https://vitejs.dev/)
[![Fal.ai](https://img.shields.io/badge/Fal.ai-VEO%203%20Fast-00D4AA.svg)](https://fal.ai/)

</div>

## âœ¨ Features

- ğŸ­ **Avatar-Based UGC**: Transform any image into a speaking video
- âš¡ **VEO 3 Fast Integration**: Powered by Google's latest video generation model via Fal.ai
- ğŸ“± **9:16 Aspect Ratio**: Perfect for TikTok, Instagram Reels, and YouTube Shorts
- ğŸµ **Audio Generation**: Creates realistic lip-sync and natural speech
- ğŸ”„ **Real-time Progress**: Live status updates during video generation
- ğŸ¨ **Modern UI**: Beautiful, responsive interface with Tailwind CSS
- âš¡ **Fast Processing**: 8-second high-quality videos in minutes

## ğŸš€ Quick Start

### Prerequisites
- Node.js 18+ 
- npm or yarn

### Installation

```bash
# Clone the repository
git clone https://github.com/FrancescoDeFi/buzzugc.git
cd buzzugc

# Install dependencies
npm install

# Start development server
npm run dev
```

Open [http://localhost:5173](http://localhost:5173) in your browser.

## ğŸ¯ How It Works

1. **Select Avatar**: Choose or upload an avatar image
2. **Enter Script**: Type the text you want the avatar to say
3. **Generate Video**: AI creates a realistic speaking video
4. **Download & Share**: Get your UGC video ready for social media

## ğŸ›  Tech Stack

- **Frontend**: React 19, TypeScript, Vite
- **Styling**: Tailwind CSS
- **AI Model**: Google VEO 3 Fast via Fal.ai
- **Video Generation**: Image-to-video with lip-sync
- **Build Tool**: Vite for fast development

## ğŸ“ Project Structure

```
buzzugc/
â”œâ”€â”€ components/           # React components
â”‚   â”œâ”€â”€ AvatarCard.tsx   # Avatar selection component
â”‚   â”œâ”€â”€ GeneratedVideo.tsx # Video display component
â”‚   â”œâ”€â”€ Header.tsx       # App header
â”‚   â””â”€â”€ LoadingIndicator.tsx # Loading states
â”œâ”€â”€ pages/               # Page components
â”‚   â”œâ”€â”€ CreationHub.tsx  # Main creation interface
â”‚   â””â”€â”€ LoginPage.tsx    # User authentication
â”œâ”€â”€ services/            # API services
â”‚   â””â”€â”€ geminiService.ts # Fal.ai VEO 3 integration
â”œâ”€â”€ types.ts            # TypeScript definitions
â”œâ”€â”€ constants.ts        # App constants
â””â”€â”€ index.tsx          # App entry point
```

## ğŸ”§ Configuration

The app is pre-configured with Fal.ai credentials. For production deployment, set up environment variables:

```bash
# Optional: Override default settings
FAL_KEY=your_fal_api_key
```

## ğŸ“Š Video Specifications

- **Duration**: 8 seconds
- **Resolution**: 720p (HD)
- **Aspect Ratio**: 9:16 (vertical)
- **Audio**: Generated with lip-sync
- **Format**: MP4
- **Frame Rate**: 24 FPS

## ğŸš€ Deployment

### Build for Production

```bash
npm run build
```

### Deploy to Vercel

```bash
# Install Vercel CLI
npm i -g vercel

# Deploy
vercel
```

### Deploy to Netlify

```bash
# Build the project
npm run build

# Deploy dist/ folder to Netlify
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [Fal.ai](https://fal.ai/) for VEO 3 Fast model access
- [Google](https://deepmind.google/technologies/veo/) for the VEO 3 model
- [React](https://reactjs.org/) and [Vite](https://vitejs.dev/) teams

---

<div align="center">
Made with â¤ï¸ for creators who want to bring their avatars to life
</div>
